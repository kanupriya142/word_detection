{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb90473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted from https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/ and saved to blackassign0001.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rising-it-cities-and-their-impact-on-the-economy-environment-infrastructure-and-city-life-in-future/ and saved to blackassign0002.txt\n",
      "Data extracted from https://insights.blackcoffer.com/internet-demands-evolution-communication-impact-and-2035s-alternative-pathways/ and saved to blackassign0003.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-in-upcoming-future/ and saved to blackassign0004.txt\n",
      "Data extracted from https://insights.blackcoffer.com/ott-platform-and-its-impact-on-the-entertainment-industry-in-future/ and saved to blackassign0005.txt\n",
      "Data extracted from https://insights.blackcoffer.com/the-rise-of-the-ott-platform-and-its-impact-on-the-entertainment-industry-by-2040/ and saved to blackassign0006.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-cyber-crime-and-its-effects/ and saved to blackassign0007.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-internet-demand-and-its-impact-on-communications-and-alternatives-by-the-year-2035-2/ and saved to blackassign0008.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040-2/ and saved to blackassign0009.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040/ and saved to blackassign0010.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-internet-demand-and-its-impact-on-communications-and-alternatives-by-the-year-2035/ and saved to blackassign0011.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/ and saved to blackassign0012.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-e-health-and-its-impact-on-humans-by-the-year-2030/ and saved to blackassign0013.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/ and saved to blackassign0014.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2/ and saved to blackassign0015.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2-2/ and saved to blackassign0016.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-chatbots-and-its-impact-on-customer-support-by-the-year-2040/ and saved to blackassign0017.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030/ and saved to blackassign0018.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-does-marketing-influence-businesses-and-consumers/ and saved to blackassign0019.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-advertisement-increase-your-market-value/ and saved to blackassign0020.txt\n",
      "Data extracted from https://insights.blackcoffer.com/negative-effects-of-marketing-on-society/ and saved to blackassign0021.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-advertisement-marketing-affects-business/ and saved to blackassign0022.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rising-it-cities-will-impact-the-economy-environment-infrastructure-and-city-life-by-the-year-2035/ and saved to blackassign0023.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-ott-platform-and-its-impact-on-entertainment-industry-by-the-year-2030/ and saved to blackassign0024.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-electric-vehicles-and-its-impact-on-livelihood-by-2040/ and saved to blackassign0025.txt\n",
      "Data extracted from https://insights.blackcoffer.com/rise-of-electric-vehicle-and-its-impact-on-livelihood-by-the-year-2040/ and saved to blackassign0026.txt\n",
      "Data extracted from https://insights.blackcoffer.com/oil-prices-by-the-year-2040-and-how-it-will-impact-the-world-economy/ and saved to blackassign0027.txt\n",
      "Data extracted from https://insights.blackcoffer.com/an-outlook-of-healthcare-by-the-year-2040-and-how-it-will-impact-human-lives/ and saved to blackassign0028.txt\n",
      "Data extracted from https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/ and saved to blackassign0029.txt\n",
      "Data extracted from https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/ and saved to blackassign0030.txt\n",
      "Data extracted from https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/ and saved to blackassign0031.txt\n",
      "Data extracted from https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/ and saved to blackassign0032.txt\n",
      "Data extracted from https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/ and saved to blackassign0033.txt\n",
      "Data extracted from https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/ and saved to blackassign0034.txt\n",
      "Data extracted from https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/ and saved to blackassign0035.txt\n",
      "Error extracting data from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Failed to extract data from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Data extracted from https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/ and saved to blackassign0037.txt\n",
      "Data extracted from https://insights.blackcoffer.com/deep-learning-impact-on-areas-of-e-learning/ and saved to blackassign0038.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-to-protect-future-data-and-its-privacy-blackcoffer/ and saved to blackassign0039.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-machines-ai-automations-and-robo-human-are-effective-in-finance-and-banking/ and saved to blackassign0040.txt\n",
      "Data extracted from https://insights.blackcoffer.com/ai-human-robotics-machine-future-planet-blackcoffer-thinking-jobs-workplace/ and saved to blackassign0041.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-ai-will-change-the-world-blackcoffer/ and saved to blackassign0042.txt\n",
      "Data extracted from https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/ and saved to blackassign0043.txt\n",
      "Data extracted from https://insights.blackcoffer.com/ai-tool-alexa-google-assistant-finance-banking-tool-future/ and saved to blackassign0044.txt\n",
      "Data extracted from https://insights.blackcoffer.com/ai-healthcare-revolution-ml-technology-algorithm-google-analytics-industrialrevolution/ and saved to blackassign0045.txt\n",
      "Data extracted from https://insights.blackcoffer.com/all-you-need-to-know-about-online-marketing/ and saved to blackassign0046.txt\n",
      "Data extracted from https://insights.blackcoffer.com/evolution-of-advertising-industry/ and saved to blackassign0047.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-data-analytics-can-help-your-business-respond-to-the-impact-of-covid-19/ and saved to blackassign0048.txt\n",
      "Error extracting data from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Failed to extract data from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Data extracted from https://insights.blackcoffer.com/environmental-impact-of-the-covid-19-pandemic-lesson-for-the-future/ and saved to blackassign0050.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/ and saved to blackassign0051.txt\n",
      "Data extracted from https://insights.blackcoffer.com/difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/ and saved to blackassign0052.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted from https://insights.blackcoffer.com/how-python-became-the-first-choice-for-data-science/ and saved to blackassign0053.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-google-fit-measure-heart-and-respiratory-rates-using-a-phone/ and saved to blackassign0054.txt\n",
      "Data extracted from https://insights.blackcoffer.com/what-is-the-future-of-mobile-apps/ and saved to blackassign0055.txt\n",
      "Data extracted from https://insights.blackcoffer.com/impact-of-ai-in-health-and-medicine/ and saved to blackassign0056.txt\n",
      "Data extracted from https://insights.blackcoffer.com/telemedicine-what-patients-like-and-dislike-about-it/ and saved to blackassign0057.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-we-forecast-future-technologies/ and saved to blackassign0058.txt\n",
      "Data extracted from https://insights.blackcoffer.com/can-robots-tackle-late-life-loneliness/ and saved to blackassign0059.txt\n",
      "Data extracted from https://insights.blackcoffer.com/embedding-care-robots-into-society-socio-technical-considerations/ and saved to blackassign0060.txt\n",
      "Data extracted from https://insights.blackcoffer.com/management-challenges-for-future-digitalization-of-healthcare-services/ and saved to blackassign0061.txt\n",
      "Data extracted from https://insights.blackcoffer.com/are-we-any-closer-to-preventing-a-nuclear-holocaust/ and saved to blackassign0062.txt\n",
      "Data extracted from https://insights.blackcoffer.com/will-technology-eliminate-the-need-for-animal-testing-in-drug-development/ and saved to blackassign0063.txt\n",
      "Data extracted from https://insights.blackcoffer.com/will-we-ever-understand-the-nature-of-consciousness/ and saved to blackassign0064.txt\n",
      "Data extracted from https://insights.blackcoffer.com/will-we-ever-colonize-outer-space/ and saved to blackassign0065.txt\n",
      "Data extracted from https://insights.blackcoffer.com/what-is-the-chance-homo-sapiens-will-survive-for-the-next-500-years/ and saved to blackassign0066.txt\n",
      "Data extracted from https://insights.blackcoffer.com/why-does-your-business-need-a-chatbot/ and saved to blackassign0067.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-you-lead-a-project-or-a-team-without-any-technical-expertise/ and saved to blackassign0068.txt\n",
      "Data extracted from https://insights.blackcoffer.com/can-you-be-great-leader-without-technical-expertise/ and saved to blackassign0069.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-does-artificial-intelligence-affect-the-environment/ and saved to blackassign0070.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes-2/ and saved to blackassign0071.txt\n",
      "Data extracted from https://insights.blackcoffer.com/is-perfection-the-greatest-enemy-of-productivity/ and saved to blackassign0072.txt\n",
      "Data extracted from https://insights.blackcoffer.com/global-financial-crisis-2008-causes-effects-and-its-solution/ and saved to blackassign0073.txt\n",
      "Data extracted from https://insights.blackcoffer.com/gender-diversity-and-equality-in-the-tech-industry/ and saved to blackassign0074.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes/ and saved to blackassign0075.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-small-business-can-survive-the-coronavirus-crisis/ and saved to blackassign0076.txt\n",
      "Data extracted from https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors-and-food-stalls/ and saved to blackassign0077.txt\n",
      "Data extracted from https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors/ and saved to blackassign0078.txt\n",
      "Data extracted from https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-tourism-aviation-industries/ and saved to blackassign0079.txt\n",
      "Data extracted from https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-sports-events-around-the-world/ and saved to blackassign0080.txt\n",
      "Data extracted from https://insights.blackcoffer.com/changing-landscape-and-emerging-trends-in-the-indian-it-ites-industry/ and saved to blackassign0081.txt\n",
      "Data extracted from https://insights.blackcoffer.com/online-gaming-adolescent-online-gaming-effects-demotivated-depression-musculoskeletal-and-psychosomatic-symptoms/ and saved to blackassign0082.txt\n",
      "Data extracted from https://insights.blackcoffer.com/human-rights-outlook/ and saved to blackassign0083.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/ and saved to blackassign0084.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-the-covid-19-crisis-is-redefining-jobs-and-services/ and saved to blackassign0085.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-to-increase-social-media-engagement-for-marketers/ and saved to blackassign0086.txt\n",
      "Data extracted from https://insights.blackcoffer.com/impacts-of-covid-19-on-streets-sides-food-stalls/ and saved to blackassign0087.txt\n",
      "Data extracted from https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets-2/ and saved to blackassign0088.txt\n",
      "Data extracted from https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-5/ and saved to blackassign0089.txt\n",
      "Data extracted from https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-4/ and saved to blackassign0090.txt\n",
      "Data extracted from https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-2/ and saved to blackassign0091.txt\n",
      "Data extracted from https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/ and saved to blackassign0092.txt\n",
      "Data extracted from https://insights.blackcoffer.com/travel-and-tourism-outlook/ and saved to blackassign0093.txt\n",
      "Data extracted from https://insights.blackcoffer.com/gaming-disorder-and-effects-of-gaming-on-health/ and saved to blackassign0094.txt\n",
      "Data extracted from https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation/ and saved to blackassign0095.txt\n",
      "Data extracted from https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation-2/ and saved to blackassign0096.txt\n",
      "Data extracted from https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-office-space-and-co-working-industries/ and saved to blackassign0097.txt\n",
      "Data extracted from https://insights.blackcoffer.com/contribution-of-handicrafts-visual-arts-literature-in-the-indian-economy/ and saved to blackassign0098.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/ and saved to blackassign0099.txt\n",
      "Data extracted from https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/ and saved to blackassign0100.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Read input.xlsx\n",
    "input_file = \"input.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract article text from URL\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract title and article text\n",
    "        title = soup.title.text.strip()\n",
    "        article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "\n",
    "        return title, article_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Iterate through URLs and extract data\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    title, article_text = extract_article_text(url)\n",
    "\n",
    "    if title and article_text:\n",
    "        # Save the extracted article in a text file\n",
    "        output_file = f\"{url_id}.txt\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {title}\\n\\n{article_text}\")\n",
    "\n",
    "        print(f\"Data extracted from {url} and saved to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to extract data from {url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f53a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)]\n",
      "Package                       Version\n",
      "----------------------------- ---------------\n",
      "absl-py                       2.0.0\n",
      "aiobotocore                   2.4.2\n",
      "aiofiles                      22.1.0\n",
      "aiohttp                       3.8.3\n",
      "aioitertools                  0.7.1\n",
      "aiosignal                     1.2.0\n",
      "aiosqlite                     0.18.0\n",
      "alabaster                     0.7.12\n",
      "anaconda-catalogs             0.2.0\n",
      "anaconda-client               1.12.0\n",
      "anaconda-navigator            2.4.2\n",
      "anaconda-project              0.11.1\n",
      "ann-visualizer                2.5\n",
      "annotated-types               0.6.0\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.3\n",
      "astroid                       2.14.2\n",
      "astropy                       5.1\n",
      "asttokens                     2.0.5\n",
      "astunparse                    1.6.3\n",
      "async-timeout                 4.0.2\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         22.1.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.11.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.12.2\n",
      "binaryornot                   0.4.4\n",
      "black                         0.0\n",
      "bleach                        4.1.0\n",
      "blis                          0.7.11\n",
      "bokeh                         3.2.1\n",
      "boltons                       23.0.0\n",
      "boto3                         1.24.28\n",
      "botocore                      1.27.59\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "cachetools                    5.3.2\n",
      "catalogue                     2.0.10\n",
      "certifi                       2023.7.22\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.0.4\n",
      "cloudpathlib                  0.16.0\n",
      "cloudpickle                   2.2.1\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.6\n",
      "colorcet                      3.0.1\n",
      "comm                          0.1.2\n",
      "conda                         23.7.2\n",
      "conda-build                   3.26.0\n",
      "conda-content-trust           0+unknown\n",
      "conda_index                   0.2.3\n",
      "conda-libmamba-solver         23.5.0\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        2.2.0\n",
      "conda_package_streaming       0.9.0\n",
      "conda-repo-cli                1.0.41\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "confection                    0.1.4\n",
      "constantly                    15.1.0\n",
      "contourpy                     1.0.5\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  41.0.2\n",
      "cssselect                     1.1.0\n",
      "cycler                        0.11.0\n",
      "cymem                         2.0.8\n",
      "cytoolz                       0.12.0\n",
      "daal4py                       2023.1.1\n",
      "dask                          2023.6.0\n",
      "datashader                    0.15.1\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.6.7\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.6\n",
      "distributed                   2023.6.0\n",
      "docstring-to-markdown         0.11\n",
      "docutils                      0.18.1\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "executing                     0.8.3\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.9.0\n",
      "flake8                        6.0.0\n",
      "Flask                         2.2.2\n",
      "flatbuffers                   23.5.26\n",
      "fonttools                     4.25.0\n",
      "frozenlist                    1.3.3\n",
      "fsspec                        2023.3.0\n",
      "future                        0.18.3\n",
      "gast                          0.5.4\n",
      "gensim                        4.3.0\n",
      "glob2                         0.7\n",
      "google-auth                   2.23.3\n",
      "google-auth-oauthlib          1.0.0\n",
      "google-pasta                  0.2.0\n",
      "graphviz                      0.20.1\n",
      "greenlet                      2.0.1\n",
      "grpcio                        1.59.2\n",
      "h5py                          3.7.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.17.0\n",
      "hvplot                        0.8.4\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.4\n",
      "imagecodecs                   2021.8.26\n",
      "imageio                       2.26.0\n",
      "imagesize                     1.4.1\n",
      "imbalanced-learn              0.10.1\n",
      "importlib-metadata            6.0.0\n",
      "incremental                   21.3.0\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "intake                        0.6.8\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.19.2\n",
      "ipython                       8.12.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    8.0.4\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  2.0.1\n",
      "jaraco.classes                3.2.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     0.9.0\n",
      "Jinja2                        3.1.2\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.2.0\n",
      "json5                         0.9.6\n",
      "jsonpatch                     1.32\n",
      "jsonpointer                   2.1\n",
      "jsonschema                    4.17.3\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                7.4.9\n",
      "jupyter-console               6.6.3\n",
      "jupyter_core                  5.3.0\n",
      "jupyter-events                0.6.3\n",
      "jupyter-server                1.23.4\n",
      "jupyter_server_fileid         0.9.0\n",
      "jupyter_server_ydoc           0.8.0\n",
      "jupyter-ydoc                  0.2.4\n",
      "jupyterlab                    3.6.3\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab_server             2.22.0\n",
      "jupyterlab-widgets            3.0.5\n",
      "kaggle                        1.5.16\n",
      "keras                         2.14.0\n",
      "keyring                       23.13.1\n",
      "kiwisolver                    1.4.4\n",
      "langcodes                     3.3.0\n",
      "lazy_loader                   0.2\n",
      "lazy-object-proxy             1.6.0\n",
      "libarchive-c                  2.9\n",
      "libclang                      16.0.6\n",
      "libmambapy                    1.4.1\n",
      "linkify-it-py                 2.0.0\n",
      "llvmlite                      0.40.0\n",
      "lmdb                          1.4.1\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.2\n",
      "lz4                           4.3.2\n",
      "Markdown                      3.4.1\n",
      "markdown-it-py                2.2.0\n",
      "MarkupSafe                    2.1.1\n",
      "matplotlib                    3.7.1\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.7.0\n",
      "mdit-py-plugins               0.3.0\n",
      "mdurl                         0.1.0\n",
      "menuinst                      1.4.19\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.6\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "ml-dtypes                     0.2.0\n",
      "mlxtend                       0.23.0\n",
      "more-itertools                8.12.0\n",
      "mpmath                        1.3.0\n",
      "msgpack                       1.0.3\n",
      "multidict                     6.0.2\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "murmurhash                    1.0.10\n",
      "mypy-extensions               0.4.3\n",
      "navigator-updater             0.4.0\n",
      "nbclassic                     0.5.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.5.4\n",
      "nbformat                      5.7.0\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      3.1\n",
      "nltk                          3.8.1\n",
      "notebook                      6.5.4\n",
      "notebook_shim                 0.2.2\n",
      "numba                         0.57.0\n",
      "numexpr                       2.8.4\n",
      "numpy                         1.24.3\n",
      "numpydoc                      1.5.0\n",
      "oauthlib                      3.2.2\n",
      "opencv-python                 4.8.1.78\n",
      "opendatasets                  0.1.22\n",
      "openpyxl                      3.0.10\n",
      "opt-einsum                    3.3.0\n",
      "packaging                     23.0\n",
      "pandas                        1.5.3\n",
      "pandocfilters                 1.5.0\n",
      "panel                         1.2.1\n",
      "param                         1.13.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.2.0\n",
      "pathlib                       1.0.1\n",
      "pathspec                      0.10.3\n",
      "patsy                         0.5.3\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.4.0\n",
      "pip                           23.2.1\n",
      "pkginfo                       1.9.6\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "ply                           3.11\n",
      "pooch                         1.4.0\n",
      "poyo                          0.5.0\n",
      "preshed                       3.0.9\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.36\n",
      "Protego                       0.1.16\n",
      "protobuf                      4.24.4\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py-cpuinfo                    8.0.0\n",
      "pyarrow                       11.0.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.10.0\n",
      "pycosat                       0.6.4\n",
      "pycparser                     2.21\n",
      "pyct                          0.5.0\n",
      "pycurl                        7.45.2\n",
      "pydantic                      2.5.3\n",
      "pydantic_core                 2.14.6\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.3.0\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      3.0.1\n",
      "Pygments                      2.15.1\n",
      "PyJWT                         2.4.0\n",
      "pylint                        2.16.2\n",
      "pylint-venv                   2.3.0\n",
      "pyls-spyder                   0.4.0\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.34\n",
      "pyOpenSSL                     23.2.0\n",
      "pyparsing                     3.0.9\n",
      "PyQt5                         5.15.7\n",
      "PyQt5-sip                     12.11.0\n",
      "PyQtWebEngine                 5.15.4\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.4.0\n",
      "python-dateutil               2.8.2\n",
      "python-json-logger            2.0.7\n",
      "python-lsp-black              1.2.1\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.7.2\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.1\n",
      "pytoolconfig                  1.2.5\n",
      "pytz                          2022.7\n",
      "pyviz-comms                   2.3.0\n",
      "PyWavelets                    1.4.1\n",
      "pywin32                       305.1\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.10\n",
      "PyYAML                        6.0\n",
      "pyzmq                         23.2.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.2.2\n",
      "QtAwesome                     1.2.2\n",
      "qtconsole                     5.4.2\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.5.0\n",
      "regex                         2022.7.9\n",
      "requests                      2.31.0\n",
      "requests-file                 1.5.1\n",
      "requests-oauthlib             1.3.1\n",
      "requests-toolbelt             1.0.0\n",
      "rfc3339-validator             0.1.4\n",
      "rfc3986-validator             0.1.1\n",
      "rope                          1.7.0\n",
      "rsa                           4.9\n",
      "Rtree                         1.0.1\n",
      "ruamel.yaml                   0.17.21\n",
      "ruamel-yaml-conda             0.17.21\n",
      "s3fs                          2023.3.0\n",
      "s3transfer                    0.6.0\n",
      "sacremoses                    0.0.43\n",
      "scikit-image                  0.20.0\n",
      "scikit-learn                  1.3.0\n",
      "scikit-learn-intelex          20230426.121932\n",
      "scipy                         1.10.1\n",
      "Scrapy                        2.8.0\n",
      "seaborn                       0.12.2\n",
      "Send2Trash                    1.8.0\n",
      "service-identity              18.1.0\n",
      "setuptools                    68.0.0\n",
      "sip                           6.6.2\n",
      "six                           1.16.0\n",
      "smart-open                    5.2.1\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.4\n",
      "spacy                         3.7.2\n",
      "spacy-legacy                  3.0.12\n",
      "spacy-loggers                 1.0.5\n",
      "Sphinx                        5.0.2\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder                        5.4.3\n",
      "spyder-kernels                2.4.3\n",
      "SQLAlchemy                    1.4.39\n",
      "srsly                         2.4.8\n",
      "stack-data                    0.2.0\n",
      "statsmodels                   0.14.0\n",
      "sympy                         1.11.1\n",
      "tables                        3.8.0\n",
      "tabulate                      0.8.10\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.2.2\n",
      "tensorboard                   2.14.1\n",
      "tensorboard-data-server       0.7.2\n",
      "tensorflow                    2.14.0\n",
      "tensorflow-estimator          2.14.0\n",
      "tensorflow-intel              2.14.0\n",
      "tensorflow-io-gcs-filesystem  0.31.0\n",
      "termcolor                     2.3.0\n",
      "terminado                     0.17.1\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "thinc                         8.2.2\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2021.7.2\n",
      "tinycss2                      1.2.1\n",
      "tldextract                    3.2.0\n",
      "toml                          0.10.2\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.12.0\n",
      "tornado                       6.3.2\n",
      "tqdm                          4.65.0\n",
      "traitlets                     5.7.1\n",
      "transformers                  2.1.1\n",
      "Twisted                       22.10.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typer                         0.9.0\n",
      "typing_extensions             4.7.1\n",
      "uc-micro-py                   1.0.1\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "urllib3                       1.26.16\n",
      "w3lib                         1.21.0\n",
      "wasabi                        1.1.2\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "weasel                        0.3.4\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      2.2.3\n",
      "whatthepatch                  1.0.2\n",
      "wheel                         0.38.4\n",
      "widgetsnbextension            4.0.5\n",
      "win-inet-pton                 1.1.0\n",
      "wrapt                         1.14.1\n",
      "xarray                        2023.6.0\n",
      "xlwings                       0.29.1\n",
      "xyzservices                   2022.9.0\n",
      "y-py                          0.5.9\n",
      "yapf                          0.31.0\n",
      "yarl                          1.8.1\n",
      "ypy-websocket                 0.8.2\n",
      "zict                          2.2.0\n",
      "zipp                          3.11.0\n",
      "zope.interface                5.4.0\n",
      "zstandard                     0.19.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753f8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 11.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 11.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 7.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.2/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.2/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.2/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 7.5 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.4/12.8 MB 8.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.8/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.2/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.9/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.8/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.2/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.0/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.7/12.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.4/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 13.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab64206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analysis completed for blackassign0001.txt\n",
      "Text analysis completed for blackassign0002.txt\n",
      "Text analysis completed for blackassign0003.txt\n",
      "Text analysis completed for blackassign0004.txt\n",
      "Text analysis completed for blackassign0005.txt\n",
      "Text analysis completed for blackassign0006.txt\n",
      "Text analysis completed for blackassign0007.txt\n",
      "Text analysis completed for blackassign0008.txt\n",
      "Text analysis completed for blackassign0009.txt\n",
      "Text analysis completed for blackassign0010.txt\n",
      "Text analysis completed for blackassign0011.txt\n",
      "Text analysis completed for blackassign0012.txt\n",
      "Text analysis completed for blackassign0013.txt\n",
      "Text analysis completed for blackassign0014.txt\n",
      "Text analysis completed for blackassign0015.txt\n",
      "Text analysis completed for blackassign0016.txt\n",
      "Text analysis completed for blackassign0017.txt\n",
      "Text analysis completed for blackassign0018.txt\n",
      "Text analysis completed for blackassign0019.txt\n",
      "Text analysis completed for blackassign0020.txt\n",
      "Text analysis completed for blackassign0021.txt\n",
      "Text analysis completed for blackassign0022.txt\n",
      "Text analysis completed for blackassign0023.txt\n",
      "Text analysis completed for blackassign0024.txt\n",
      "Text analysis completed for blackassign0025.txt\n",
      "Text analysis completed for blackassign0026.txt\n",
      "Text analysis completed for blackassign0027.txt\n",
      "Text analysis completed for blackassign0028.txt\n",
      "Text analysis completed for blackassign0029.txt\n",
      "Text analysis completed for blackassign0030.txt\n",
      "Text analysis completed for blackassign0031.txt\n",
      "Text analysis completed for blackassign0032.txt\n",
      "Text analysis completed for blackassign0033.txt\n",
      "Text analysis completed for blackassign0034.txt\n",
      "Text analysis completed for blackassign0035.txt\n",
      "Error analyzing text for blackassign0036.txt: [Errno 2] No such file or directory: 'blackassign0036.txt'\n",
      "Text analysis completed for blackassign0037.txt\n",
      "Text analysis completed for blackassign0038.txt\n",
      "Text analysis completed for blackassign0039.txt\n",
      "Text analysis completed for blackassign0040.txt\n",
      "Text analysis completed for blackassign0041.txt\n",
      "Text analysis completed for blackassign0042.txt\n",
      "Text analysis completed for blackassign0043.txt\n",
      "Text analysis completed for blackassign0044.txt\n",
      "Text analysis completed for blackassign0045.txt\n",
      "Text analysis completed for blackassign0046.txt\n",
      "Text analysis completed for blackassign0047.txt\n",
      "Text analysis completed for blackassign0048.txt\n",
      "Error analyzing text for blackassign0049.txt: [Errno 2] No such file or directory: 'blackassign0049.txt'\n",
      "Text analysis completed for blackassign0050.txt\n",
      "Text analysis completed for blackassign0051.txt\n",
      "Text analysis completed for blackassign0052.txt\n",
      "Text analysis completed for blackassign0053.txt\n",
      "Text analysis completed for blackassign0054.txt\n",
      "Text analysis completed for blackassign0055.txt\n",
      "Text analysis completed for blackassign0056.txt\n",
      "Text analysis completed for blackassign0057.txt\n",
      "Text analysis completed for blackassign0058.txt\n",
      "Text analysis completed for blackassign0059.txt\n",
      "Text analysis completed for blackassign0060.txt\n",
      "Text analysis completed for blackassign0061.txt\n",
      "Text analysis completed for blackassign0062.txt\n",
      "Text analysis completed for blackassign0063.txt\n",
      "Text analysis completed for blackassign0064.txt\n",
      "Text analysis completed for blackassign0065.txt\n",
      "Text analysis completed for blackassign0066.txt\n",
      "Text analysis completed for blackassign0067.txt\n",
      "Text analysis completed for blackassign0068.txt\n",
      "Text analysis completed for blackassign0069.txt\n",
      "Text analysis completed for blackassign0070.txt\n",
      "Text analysis completed for blackassign0071.txt\n",
      "Text analysis completed for blackassign0072.txt\n",
      "Text analysis completed for blackassign0073.txt\n",
      "Text analysis completed for blackassign0074.txt\n",
      "Text analysis completed for blackassign0075.txt\n",
      "Text analysis completed for blackassign0076.txt\n",
      "Text analysis completed for blackassign0077.txt\n",
      "Text analysis completed for blackassign0078.txt\n",
      "Text analysis completed for blackassign0079.txt\n",
      "Text analysis completed for blackassign0080.txt\n",
      "Text analysis completed for blackassign0081.txt\n",
      "Text analysis completed for blackassign0082.txt\n",
      "Text analysis completed for blackassign0083.txt\n",
      "Text analysis completed for blackassign0084.txt\n",
      "Text analysis completed for blackassign0085.txt\n",
      "Text analysis completed for blackassign0086.txt\n",
      "Text analysis completed for blackassign0087.txt\n",
      "Text analysis completed for blackassign0088.txt\n",
      "Text analysis completed for blackassign0089.txt\n",
      "Text analysis completed for blackassign0090.txt\n",
      "Text analysis completed for blackassign0091.txt\n",
      "Text analysis completed for blackassign0092.txt\n",
      "Text analysis completed for blackassign0093.txt\n",
      "Text analysis completed for blackassign0094.txt\n",
      "Text analysis completed for blackassign0095.txt\n",
      "Text analysis completed for blackassign0096.txt\n",
      "Text analysis completed for blackassign0097.txt\n",
      "Text analysis completed for blackassign0098.txt\n",
      "Text analysis completed for blackassign0099.txt\n",
      "Text analysis completed for blackassign0100.txt\n",
      "Text analysis results saved to text_analysis_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to perform textual analysis and compute variables\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Compute variables\n",
    "    word_count = len(doc)\n",
    "    sentence_count = len(list(doc.sents))\n",
    "    unique_words = len(set([token.text.lower() for token in doc if token.is_alpha]))\n",
    "    avg_word_length = sum(len(token) for token in doc if token.is_alpha) / word_count if word_count > 0 else 0\n",
    "\n",
    "    return word_count, sentence_count, unique_words, avg_word_length\n",
    "\n",
    "# Read the output structure file\n",
    "output_structure_file = \"Output Data Structure.xlsx\"\n",
    "output_df = pd.read_excel(output_structure_file)\n",
    "\n",
    "# Iterate through the extracted text files\n",
    "for index, row in output_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    file_path = f\"{url_id}.txt\"\n",
    "\n",
    "    try:\n",
    "        # Read the content of the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            article_content = file.read()\n",
    "\n",
    "        # Perform textual analysis and compute variables\n",
    "        word_count, sentence_count, unique_words, avg_word_length = analyze_text(article_content)\n",
    "\n",
    "        # Update the output DataFrame with computed variables\n",
    "        output_df.at[index, 'Word_Count'] = word_count\n",
    "        output_df.at[index, 'Sentence_Count'] = sentence_count\n",
    "        output_df.at[index, 'Unique_Words'] = unique_words\n",
    "        output_df.at[index, 'Avg_Word_Length'] = avg_word_length\n",
    "\n",
    "        print(f\"Text analysis completed for {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing text for {file_path}: {e}\")\n",
    "\n",
    "# Save the updated output DataFrame to a new Excel file\n",
    "output_df.to_excel(\"text_analysis_output.xlsx\", index=False)\n",
    "print(\"Text analysis results saved to text_analysis_output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12637417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eebd1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyphen\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.0 MB 1.7 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.2/2.0 MB 2.8 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.4/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.5/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.8/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.0/2.0 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.1/2.0 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.3/2.0 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.3/2.0 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.3/2.0 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.5/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.8/2.0 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.8/2.0 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.0/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pyphen\n",
      "Successfully installed pyphen-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyphen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7803a1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (2022.7.9)\n",
      "Requirement already satisfied: textblob in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Collecting syllables\n",
      "  Obtaining dependency information for syllables from https://files.pythonhosted.org/packages/27/c5/96e282163836a83d9f0cfdc5792e5bbb8baab18ee2e2dca4a85875588f1b/syllables-1.0.9-py3-none-any.whl.metadata\n",
      "  Downloading syllables-1.0.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: click in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
      "  Obtaining dependency information for cmudict<2.0.0,>=1.0.11 from https://files.pythonhosted.org/packages/9c/8a/3e16710d858c61232f3ac5423c76d2dae112e77e7e7433b036711382e69e/cmudict-1.0.16-py3-none-any.whl.metadata\n",
      "  Downloading cmudict-1.0.16-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=5.1 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from syllables) (6.0.0)\n",
      "Collecting importlib-resources>=5 (from cmudict<2.0.0,>=1.0.11->syllables)\n",
      "  Obtaining dependency information for importlib-resources>=5 from https://files.pythonhosted.org/packages/93/e8/facde510585869b5ec694e8e0363ffe4eba067cb357a8398a55f6a1f8023/importlib_resources-6.1.1-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from importlib-metadata<7.0,>=5.1->syllables) (3.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading syllables-1.0.9-py3-none-any.whl (15 kB)\n",
      "Downloading cmudict-1.0.16-py3-none-any.whl (939 kB)\n",
      "   ---------------------------------------- 0.0/939.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 92.2/939.4 kB 2.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 225.3/939.4 kB 3.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 225.3/939.4 kB 3.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 460.8/939.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 727.0/939.4 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  931.8/939.4 kB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 939.4/939.4 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: importlib-resources, cmudict, syllables\n",
      "Successfully installed cmudict-1.0.16 importlib-resources-6.1.1 syllables-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk regex textblob syllables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f2f506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "     ---------------------------------------- 0.0/105.1 kB ? eta -:--:--\n",
      "     -------------------------------------  102.4/105.1 kB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 105.1/105.1 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyphen in c:\\users\\kannu priya\\anaconda3\\lib\\site-packages (from textstat) (0.14.0)\n",
      "Installing collected packages: textstat\n",
      "Successfully installed textstat-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66a6c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Kannu Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac957dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analysis completed for blackassign0001.txt\n",
      "Text analysis completed for blackassign0002.txt\n",
      "Text analysis completed for blackassign0003.txt\n",
      "Text analysis completed for blackassign0004.txt\n",
      "Text analysis completed for blackassign0005.txt\n",
      "Text analysis completed for blackassign0006.txt\n",
      "Text analysis completed for blackassign0007.txt\n",
      "Text analysis completed for blackassign0008.txt\n",
      "Text analysis completed for blackassign0009.txt\n",
      "Text analysis completed for blackassign0010.txt\n",
      "Text analysis completed for blackassign0011.txt\n",
      "Text analysis completed for blackassign0012.txt\n",
      "Text analysis completed for blackassign0013.txt\n",
      "Text analysis completed for blackassign0014.txt\n",
      "Text analysis completed for blackassign0015.txt\n",
      "Text analysis completed for blackassign0016.txt\n",
      "Text analysis completed for blackassign0017.txt\n",
      "Text analysis completed for blackassign0018.txt\n",
      "Text analysis completed for blackassign0019.txt\n",
      "Text analysis completed for blackassign0020.txt\n",
      "Text analysis completed for blackassign0021.txt\n",
      "Text analysis completed for blackassign0022.txt\n",
      "Text analysis completed for blackassign0023.txt\n",
      "Text analysis completed for blackassign0024.txt\n",
      "Text analysis completed for blackassign0025.txt\n",
      "Text analysis completed for blackassign0026.txt\n",
      "Text analysis completed for blackassign0027.txt\n",
      "Text analysis completed for blackassign0028.txt\n",
      "Text analysis completed for blackassign0029.txt\n",
      "Text analysis completed for blackassign0030.txt\n",
      "Text analysis completed for blackassign0031.txt\n",
      "Text analysis completed for blackassign0032.txt\n",
      "Text analysis completed for blackassign0033.txt\n",
      "Text analysis completed for blackassign0034.txt\n",
      "Text analysis completed for blackassign0035.txt\n",
      "Error analyzing text for blackassign0036.txt: [Errno 2] No such file or directory: 'blackassign0036.txt'\n",
      "Text analysis completed for blackassign0037.txt\n",
      "Text analysis completed for blackassign0038.txt\n",
      "Text analysis completed for blackassign0039.txt\n",
      "Text analysis completed for blackassign0040.txt\n",
      "Text analysis completed for blackassign0041.txt\n",
      "Text analysis completed for blackassign0042.txt\n",
      "Text analysis completed for blackassign0043.txt\n",
      "Text analysis completed for blackassign0044.txt\n",
      "Text analysis completed for blackassign0045.txt\n",
      "Text analysis completed for blackassign0046.txt\n",
      "Text analysis completed for blackassign0047.txt\n",
      "Text analysis completed for blackassign0048.txt\n",
      "Error analyzing text for blackassign0049.txt: [Errno 2] No such file or directory: 'blackassign0049.txt'\n",
      "Text analysis completed for blackassign0050.txt\n",
      "Text analysis completed for blackassign0051.txt\n",
      "Text analysis completed for blackassign0052.txt\n",
      "Text analysis completed for blackassign0053.txt\n",
      "Text analysis completed for blackassign0054.txt\n",
      "Text analysis completed for blackassign0055.txt\n",
      "Text analysis completed for blackassign0056.txt\n",
      "Text analysis completed for blackassign0057.txt\n",
      "Text analysis completed for blackassign0058.txt\n",
      "Text analysis completed for blackassign0059.txt\n",
      "Text analysis completed for blackassign0060.txt\n",
      "Text analysis completed for blackassign0061.txt\n",
      "Text analysis completed for blackassign0062.txt\n",
      "Text analysis completed for blackassign0063.txt\n",
      "Text analysis completed for blackassign0064.txt\n",
      "Text analysis completed for blackassign0065.txt\n",
      "Text analysis completed for blackassign0066.txt\n",
      "Text analysis completed for blackassign0067.txt\n",
      "Text analysis completed for blackassign0068.txt\n",
      "Text analysis completed for blackassign0069.txt\n",
      "Text analysis completed for blackassign0070.txt\n",
      "Text analysis completed for blackassign0071.txt\n",
      "Text analysis completed for blackassign0072.txt\n",
      "Text analysis completed for blackassign0073.txt\n",
      "Text analysis completed for blackassign0074.txt\n",
      "Text analysis completed for blackassign0075.txt\n",
      "Text analysis completed for blackassign0076.txt\n",
      "Text analysis completed for blackassign0077.txt\n",
      "Text analysis completed for blackassign0078.txt\n",
      "Text analysis completed for blackassign0079.txt\n",
      "Text analysis completed for blackassign0080.txt\n",
      "Text analysis completed for blackassign0081.txt\n",
      "Text analysis completed for blackassign0082.txt\n",
      "Text analysis completed for blackassign0083.txt\n",
      "Text analysis completed for blackassign0084.txt\n",
      "Text analysis completed for blackassign0085.txt\n",
      "Text analysis completed for blackassign0086.txt\n",
      "Text analysis completed for blackassign0087.txt\n",
      "Text analysis completed for blackassign0088.txt\n",
      "Text analysis completed for blackassign0089.txt\n",
      "Text analysis completed for blackassign0090.txt\n",
      "Text analysis completed for blackassign0091.txt\n",
      "Text analysis completed for blackassign0092.txt\n",
      "Text analysis completed for blackassign0093.txt\n",
      "Text analysis completed for blackassign0094.txt\n",
      "Text analysis completed for blackassign0095.txt\n",
      "Text analysis completed for blackassign0096.txt\n",
      "Text analysis completed for blackassign0097.txt\n",
      "Text analysis completed for blackassign0098.txt\n",
      "Text analysis completed for blackassign0099.txt\n",
      "Text analysis completed for blackassign0100.txt\n",
      "Text analysis results saved to text_analysis_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Registering the 'syllables' extension attribute\n",
    "Token.set_extension('syllables', default=None, force=True)\n",
    "\n",
    "# Function to clean text using stop words\n",
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = ' '.join([token.text for token in doc if token.text.lower() not in stop_words])\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to create a dictionary of positive and negative words\n",
    "def create_sentiment_dictionary():\n",
    "    # You can customize the positive and negative word lists based on your requirements\n",
    "    positive_words = ['good', 'happy', 'positive']\n",
    "    negative_words = ['bad', 'sad', 'negative']\n",
    "    \n",
    "    sentiment_dict = {'positive': positive_words, 'negative': negative_words}\n",
    "    return sentiment_dict\n",
    "\n",
    "# Function to extract derived variables\n",
    "def extract_derived_variables(text):\n",
    "    # Add your logic to extract derived variables based on the text content\n",
    "    # For example, you can count the occurrences of specific phrases or patterns\n",
    "    derived_variable_1 = text.count('example_phrase_1')\n",
    "    derived_variable_2 = text.count('example_phrase_2')\n",
    "\n",
    "    return derived_variable_1, derived_variable_2\n",
    "\n",
    "# Function to perform textual analysis and compute variables\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Compute variables\n",
    "    word_count = len(doc)\n",
    "    sentence_count = len(list(doc.sents))\n",
    "    unique_words = len(set([token.text.lower() for token in doc if token.is_alpha]))\n",
    "    avg_word_length = sum(len(token) for token in doc if token.is_alpha) / word_count if word_count > 0 else 0\n",
    "\n",
    "    # Sentiment analysis using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "\n",
    "    # Additional features\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    complex_word_count = sum(1 for token in doc if token.is_alpha and len(token) > 2)  # Adjust the condition for complexity\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count > 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Count personal pronouns\n",
    "    personal_pronouns = sum(1 for token in doc if token.pos_ == 'PRON' and token.text.lower() in ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'])\n",
    "\n",
    "    # Syllable count per word\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            token._.syllables = count_syllables(token.text)\n",
    "\n",
    "    syllable_per_word = sum(token._.syllables for token in doc if token.is_alpha) / word_count if word_count > 0 else 0\n",
    "\n",
    "    return word_count, sentence_count, unique_words, avg_word_length, \\\n",
    "           polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, \\\n",
    "           fog_index, complex_word_count, syllable_per_word, personal_pronouns\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    # Add your syllable counting logic here\n",
    "    # This is a simple example and may not cover all cases\n",
    "    # You might want to use a more comprehensive method\n",
    "    vowels = 'aeiouy'\n",
    "    count = 0\n",
    "    word = word.lower().strip(\".:;?!\")\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Read the output structure file\n",
    "output_structure_file = \"Output Data Structure.xlsx\"\n",
    "output_df = pd.read_excel(output_structure_file)\n",
    "\n",
    "# Iterate through the extracted text files\n",
    "for index, row in output_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    file_path = f\"{url_id}.txt\"\n",
    "\n",
    "    try:\n",
    "        # Read the content of the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            article_content = file.read()\n",
    "\n",
    "        # Clean text using stop words\n",
    "        cleaned_content = clean_text(article_content)\n",
    "\n",
    "        # Perform textual analysis and compute variables\n",
    "        word_count, sentence_count, unique_words, avg_word_length, \\\n",
    "        polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, \\\n",
    "        fog_index, complex_word_count, syllable_per_word, personal_pronouns = analyze_text(cleaned_content)\n",
    "\n",
    "        # Create sentiment dictionary\n",
    "        sentiment_dict = create_sentiment_dictionary()\n",
    "\n",
    "        # Extract derived variables\n",
    "        derived_variable_1, derived_variable_2 = extract_derived_variables(cleaned_content)\n",
    "\n",
    "        # Update the output DataFrame with computed variables\n",
    "        output_df.at[index, 'Word_Count'] = word_count\n",
    "        output_df.at[index, 'Sentence_Count'] = sentence_count\n",
    "        output_df.at[index, 'Unique_Words'] = unique_words\n",
    "        output_df.at[index, 'Avg_Word_Length'] = avg_word_length\n",
    "        output_df.at[index, 'Derived_Variable_1'] = derived_variable_1\n",
    "        output_df.at[index, 'Derived_Variable_2'] = derived_variable_2\n",
    "        output_df.at[index, 'Polarity_Score'] = polarity_score\n",
    "        output_df.at[index, 'Subjectivity_Score'] = subjectivity_score\n",
    "        output_df.at[index, 'Avg_Sentence_Length'] = avg_sentence_length\n",
    "        output_df.at[index, 'Percentage_Complex_Words'] = percentage_complex_words\n",
    "        output_df.at[index, 'Fog_Index'] = fog_index\n",
    "        output_df.at[index, 'Complex_Word_Count'] = complex_word_count\n",
    "        output_df.at[index, 'Syllable_Per_Word'] = syllable_per_word\n",
    "        output_df.at[index, 'Personal_Pronouns'] = personal_pronouns\n",
    "\n",
    "        print(f\"Text analysis completed for {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing text for {file_path}: {e}\")\n",
    "\n",
    "# Save the updated output DataFrame to a new Excel file\n",
    "output_df.to_excel(\"text_analysis_output.xlsx\", index=False)\n",
    "print(\"Text analysis results saved to text_analysis_output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d3a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
